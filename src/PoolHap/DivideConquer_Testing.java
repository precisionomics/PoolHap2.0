package PoolHap;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Calendar;
import java.util.HashMap;
import java.util.HashSet;

import java.util.LinkedHashMap;
import java.util.Date;

import PoolHap.Parameters.*;
import PoolHap.HapConfig;

import static java.util.stream.Collectors.*;
import static java.util.Map.Entry.*;

public class DivideConquer {

	/*
	 * @author  Quan Long. Oct 08, 2018
	 * 
	 * This class divides the genome into multiple regions, reconstruct haplotypes from each region, 
	 * and combine the results. Recursively, the above procedure will be done multiple times.    
	 * 
	 * There are two strategies on how to divide the genome: 
	 * (1) specify a fixed number of genetic markers (SNPs) for each region
	 * (2) calculate the number of genetic markers in each region based on estimated LD structure
	 * 		The LD structure can be estimated by :
	 * 		(2.1) Another reference panel
	 * 		(2.2) Information from sequencing reads.  
	 * Different layers of Divide-and-Conquer may adopt different strategies. 
	 * 
	 * All parameters are in the Object "parameters" to facilitate Machine Learning-based training. 
	 */
	
	
	
	public DivideParameters divide_parameters;
	
	public int num_pools;
	public int num_sites;
	public int num_regsion_level_I;
	public int[][] regions_level_I;    // num_regsion_level_I x 2 (start and end)
	public int num_regsion_level_II;
	public int[][] regions_level_II;   // num_regsion_level_II x 2 (start and end)

	// Note that we do NOT generate the fields for HapConfigs of each regions. These will be generated on-the-fly during the calculation.
	public SiteInPoolFreqAnno in_pool_sites_freq_anno;   // the in-pool frequencies and annotations for all sites.
	public String[][] gc_outcome;
	
	/*
	 * Constructor that generate dividing plans based on GC outcomes.
	 */
	public DivideConquer(String frequency_file, String gc_pathes_file, String parameter_file, String dc_plan_outfile){
		try{
			this.divide_parameters=new DivideParameters(parameter_file);
			System.out.println("Finished loading PHX Parameter File: "+parameter_file);
			load_gc_outcome(parse_gc_pathes_file(gc_pathes_file)); 
			System.out.println("Finished loading Graph-coloring Files: "+gc_pathes_file);
			System.out.println("#pools="+this.num_pools+"\t#sites="+this.num_sites);
			generate_dividing_plan_two_level(this.gc_outcome);
			output_current_DC_plan(dc_plan_outfile);
			System.out.println("Finished generating divide & conquer plan. The plan has been written to "+dc_plan_outfile);
			System.out.println("The current dividing plan:");
			this.output_current_DC_plan();
			this.in_pool_sites_freq_anno=new SiteInPoolFreqAnno(frequency_file);
		}catch(Exception e){e.printStackTrace();}
	}
	
	/*
	 * Load another dividing plan generated by other methods, e.g., LD. //TODO
	 * 
	 * The plan might be loaded from a file that records something learned by another algorithm.
	 * The format of that file is as follows:
	 * 
	 *
	 */
	public void load_dividing_plan(String dividing_plan_file){
		try{
			//TODO
		}catch(Exception e){e.printStackTrace();}
	}
	
	public String[] parse_gc_pathes_file(String gc_pathes_file){
		ArrayList<String> pathes=new ArrayList<String>();
		try{
			BufferedReader br=new BufferedReader(new FileReader(gc_pathes_file));
			String line=br.readLine();
			while(line.startsWith("#")){ // skip headers, if any
				line=br.readLine();
			}
			
			while(line!=null){
				pathes.add(line);
				line=br.readLine();
			}
			this.num_pools=pathes.size();
			br.close();
		}catch(Exception e){e.printStackTrace();}
		String[] gc_pathes=new String[this.num_pools];
		for(int p=0;p<this.num_pools;p++){
			gc_pathes[p]=pathes.get(p);
		}return gc_pathes;
	}
	/*
	 * This function designs the dividing plan solely based on the gaps in the outcome of GC 
	 * without considering the LD structure.
	 * 
	 *  The outcome will be two DC plans: 
	 *  	(1) one of them use the gaps of GC as boundaries; 
	 *  	(2) the other put all gaps within a region. 
	 *  
	 *  Parameters:
	 *  	int min_level_I_region_size;
	 *		int max_level_I_region_size;
	 *		int min_level_II_region_size; 	(Planned, but not used in its current implementation as of Nov 28th 2018.)
	 *		int max_level_II_region_size;	(Planned, but not used in its current implementation as of Nov 28th 2018.)
	 *
	 *  //TODO: there is a bug in this function if there is no gap (which is meaningless, though). Will fix this when I get time!
	 */
	
	public void generate_dividing_plan_two_level(String[][] graph_coloring_outcome){
		// Step 1) Generate list of positions where there are linkage uncertainties i.e.: gaps.
		int[] gap_positions=identify_gaps(graph_coloring_outcome);  
		// Step 2) Make the level 1 regions i.e.: windows of variants based on the generated gaps.
		ArrayList<Integer> region_cuts=new ArrayList<Integer>();	// These are the boundaries of the regions/windows of variants. 
		int curr_unmatched_len=0;	// This is the length of (number of variants in) the region/windows of variants so far.
		for(int gap=0;gap<gap_positions.length;gap++){	// For each gap...
			int previous_gap_position=(gap==0)?0:gap_positions[gap-1];	// The start position of the region currently being made. 
			int gap_len=gap_positions[gap]-previous_gap_position;		// The number of variants in the region currently being made.
			// If the region currently being made contains enough variants to fall within the acceptable range, start the next region. 
			// TODO D&C scheme need a graphic in the paper.
			if(inbetween(curr_unmatched_len+gap_len, divide_parameters.min_level_I_region_size, divide_parameters.max_level_I_region_size)){
				// create the region
				curr_unmatched_len=0;
				region_cuts.add(gap_positions[gap]);
			// ...otherwise, if the region currently being made is larger than the max allowable size, split it up so that everything after the max size is added to the next region.
			}else if(curr_unmatched_len+gap_len> divide_parameters.max_level_I_region_size){
				curr_unmatched_len=curr_unmatched_len+gap_len-divide_parameters.max_level_I_region_size;
				region_cuts.add(previous_gap_position+divide_parameters.max_level_I_region_size);
			}else { // curr_unmatched_len+gap_len < parameters.min_level_I_region_size
				// extend the current unmatched length; do not form a region
				curr_unmatched_len+=gap_len;
			}
		}
		this.num_regsion_level_I=region_cuts.size()+1;
		this.regions_level_I=new int[num_regsion_level_I][2];	// The start and end positions of each region.  
		this.regions_level_I[0][0]=0;
		for(int r=0;r<num_regsion_level_I-1;r++){	// Note that because there are max region sizes, not every region will start and/or end at a gap position. 
			this.regions_level_I[r][1]=region_cuts.get(r)-1;	// Gap positions are merely guidelines to determine linkage regions.
			this.regions_level_I[r+1][0]=region_cuts.get(r);
		}this.regions_level_I[num_regsion_level_I-1][1]=this.num_sites-1;
		// Step 3) Make the level 2 regions i.e.: windows of variants that overlap with the end of the ith level 1 window and the start of the (i + 1)th level 1 window.
		this.num_regsion_level_II=this.num_regsion_level_I-1;
		this.regions_level_II=new int[num_regsion_level_II][2];
		this.regions_level_II[0][0]=(this.regions_level_I[0][1] - this.regions_level_I[0][0]) / 2;	// !!! This is the midpoint of the 0th level 1 region.
		for(int r=0;r<num_regsion_level_II-1;r++){
			this.regions_level_II[r][1]=(this.regions_level_I[r+1][0]+this.regions_level_I[r+1][1])/2;
			this.regions_level_II[r+1][0]=this.regions_level_II[r][1]+1;	// The endpoint of the 0th level 2 region is the midpoint of the 1st level 1 region. 
		}this.regions_level_II[num_regsion_level_II-1][1]= (this.num_sites + this.regions_level_II[num_regsion_level_II-1][0]) / 2;	// !!!
	}
	
	public boolean inbetween(int value, int min, int max){
		return (value>=min && value <=max);
	}
	
	/*
	 * identify gaps from the GC outcome. 
	 * The GC outcome file is composed of inferred haplotypes in the format of:
	 * 		0-1-0-1-0?1-0-1
	 * where 0/1 stands for alleles, "-" stands for linked, and "?" stands for gap. 
	 * 
	 * There are two criteria:
	 * 	(1) the gap in a single pool, largely due to the sequencing gap
	 * 	(2) the shared gap in all the pools, largely due to the long physical distance between two sites. 
	 * 
	 * Parameters involved: 
	 * 	double gap_all_pool_cutoff
	 * 	double gap_inpool_cutoff
	 */
	public int[] identify_gaps(String[][] graph_coloring_outcome){
		ArrayList<Integer> gap_indexes=new ArrayList<>();
		// Step 1) Count up the number of gaps within each and between all of the raw GC haplotypes and all of the pools.
		int[][] gap_counts=new int[this.num_pools][this.num_sites-1]; // The count at each potential gap in each pool. "this.num_sites-1" potential gaps.
		int[] gap_counts_all=new int[this.num_sites-1];	// The cumulative count at each potential gap position. 
		double[] num_haps_inpool=new double[this.num_pools];	// The number of types of raw GC haplotypes in each pool. 
		double num_haps_all= 0.0;
		for(int p=0;p<this.num_pools;p++){	// For each pool...
			String[] haps=graph_coloring_outcome[p];	// The list of raw GC haplotypes.
			num_haps_all+=haps.length;
			num_haps_inpool[p]=haps.length;
			for(int h=0;h<haps.length;h++){	// ...for each raw GC haplotype...
				for(int k=0;k<this.num_sites-1;k++){ // ...for each potential gap... 
					if(haps[h].charAt(k*2+1)=='?'){	// If the linkage between the two variant positions is uncertain i.e.: gap is present...
						gap_counts[p][k]++;	// ...increment the count within-pool and globally. 
						gap_counts_all[k]++;
					}
				}
			}
		}
		// Step 2) Check if the potential gaps meet the within- OR between-pool frequency thresholds.
		HashSet<Integer> gap_indexes_set=new HashSet<Integer>();
		for(int k=0;k<this.num_sites-1;k++){
			if(gap_counts_all[k]/num_haps_all>=this.divide_parameters.gap_all_pool_cutoff && !gap_indexes_set.contains(k)){
				gap_indexes.add(k);
				gap_indexes_set.add(k);
			}
			for(int p=0;p<this.num_pools;p++){
				if(gap_counts[p][k]/num_haps_inpool[p]>=this.divide_parameters.gap_inpool_cutoff && !gap_indexes_set.contains(k)){	
					gap_indexes.add(k);
					gap_indexes_set.add(k);
				}
			}
		}
		gap_indexes.add(this.num_sites-1); // add the last site index as the final "gap" so that all the sites are within gaps. 
		gap_indexes_set.add(this.num_sites-1); // this is not useful for the moment, but keep the data integrity for potential future use. 
		// clean the outcome and return
		int[] gap_indexes_array=new int[gap_indexes.size()];
		for(int i=0;i<gap_indexes.size();i++){
			gap_indexes_array[i]=gap_indexes.get(i);
		}
		Arrays.sort(gap_indexes_array); // sort the indexes. 
		return gap_indexes_array;
	}
	
	/*
	 * Read files recording the outcome of GC:
	 * 
	 * 0?0-0-0?1-0-0-0-0-0-0-1-0-0-0-0-0?0-0-0?1?0?0-0-0-1-0-0?0-0
	 * 0?0-0-0?1-0-0-0-0-0-0-1-0-1-1-0-1?0-0-0?1?0?0-0-0-0-0-0?1-1
	 * 1?1-0-1?1-0-0-0-0-0-0-1-0-1-1-0-1?0-1-0?0?0?0-1-0-0-0-0?1-1
	 * 
	 * It will return GC outcomes in the form of strings.
	 */
	public void load_gc_outcome(String[] graph_coloring_outcome_files){ 
		this.num_pools=graph_coloring_outcome_files.length;
		this.gc_outcome=new String[num_pools][];
		try{			
			for(int p_index=0;p_index<num_pools;p_index++){
				ArrayList<String> haps_list=new ArrayList<String>();
				BufferedReader br= new BufferedReader(new FileReader(new File(graph_coloring_outcome_files[p_index])));
				String line=br.readLine();
				while(line.startsWith("#"))line=br.readLine();
				this.num_sites=(line.length()+1)/2;
				while(line!=null){
					haps_list.add(line);
					line=br.readLine();
				}
				gc_outcome[p_index]=new String[haps_list.size()];
				for(int h=0;h<haps_list.size();h++){
					gc_outcome[p_index][h]=haps_list.get(h);
				}
				br.close();
			}
		}catch(Exception e){e.printStackTrace();}
	}
		
//	/*
//	 * After generating the division plan, before running the statistical analysis, 
//	 * collect the frequencies and GC and generate the related HapConfig objects.
//	 */
//	public void preprocess(){
//		
//	}
	
	

	/*
	 * Generate a HapConfig object based on the region, ready for AEM/rjMCMC or other analysis.
	 * Most structures are empty. But the priori haplotypes from GC will be formed. This helps the downstream AME/rjMCMC.  
	 * 
	 * For the gaps in the GC outcome, we will form all the possible haplotypes using the following algorithm:
	 * 
	 * (1) Initiation: all the segments (separated by "?" in the GC hap-string) will be collected. 
	 * (2) Then, iteratively, the segments who has a starting point as "0" will search for the next segment. 
	 * 		The newly combined larger segments will be put to the collection. The old segments (who have starting point 0) 
	 * 		that have been extensively matched will	be removed. 
	 * 	#Note that the segment that can extend the initial segments won't be removed until finally as they may match 
	 * 	#other initial segments in the future. 		
	 * (3) Finally, all the segments in the collection that have the full length are the outcome.    
	 * 
	 * //TODO This function doesn't allow two  '?' together, which should not be observed if GC functions well.
	 *  
	 */
	// dc_tester.analyze_regions(regions = dc_tester.regions_level_I);
	// HapConfig hap_config= generate_hapconfig_from_gc(regions, r, this.gc_outcome, this.in_pool_sites_freq_anno);
	public HapConfig generate_hapconfig_from_gc(int[][] the_region, int region_index, String[][] gc_outcome2, SiteInPoolFreqAnno in_pool_sites2) throws IOException {
		HashMap<HapSegment, Integer> all_full_haps=new HashMap<HapSegment, Integer>(); 	// Each current zero-starting sub-haplotype and their counts. Spans all pools.
//		HashMap<HapSegment, Integer> segments_buffer2=new HashMap<HapSegment, Integer>(); 
//		HashMap<HapSegment, Integer> matching_segments=new HashMap<HapSegment, Integer>(); 
		// Step 1) Initialize all possible zero-starting sub-haplotypes in region r and non-zero-starting i.e.: matching segments.
		// BufferedWriter stdout = new BufferedWriter(new FileWriter("C:\\gc\\dc_stdout_" + region_index + ".txt")); 
		int region_start=the_region[region_index][0];
		int region_end=the_region[region_index][1];
		for(int p_index=0;p_index<this.num_pools;p_index++){	// For each pool...
			HashMap<HapSegment, Integer> segments_buffer1=new HashMap<HapSegment, Integer>(); // Storage for partially finished sub-haplotypes.
			HashMap<HapSegment, Integer> segments_buffer2=new HashMap<HapSegment, Integer>(); // More storage for partially finished sub-haplotypes.
			HashMap<HapSegment, Integer> matching_segments=new HashMap<HapSegment, Integer>(); 	// Available non-zero-starting segments to finish sub-haplotypes with.
			// Step 1a) Chunk the raw GC full haplotypes into segments, and place those segments in the proper storage (zero-start vs. non-zero start). 
			for(int h=0;h<gc_outcome2[p_index].length;h++){	// For each raw GC haplotype in the pool...
				int the_seg_start=region_start;
				int the_seg_end=region_start;
				String the_seg_seq=""+gc_outcome2[p_index][h].charAt(region_start*2);	// This is the linkage indicator i.e.: ? or -.
				for(int k=region_start;k<region_end;k++){
					if(gc_outcome2[p_index][h].charAt(k*2+1)=='?'){  // There is a gap, so it's the end of a segment. 
						if(the_seg_start==region_start){	// If the segment started at the beginning of the region, it's a zero-start segment. 
							HapSegment.add2hashmap(segments_buffer1, new HapSegment(the_seg_start, the_seg_end, the_seg_seq));
						}else{	// Otherwise, it's a 'matching' segment and may be joined up to a zero-start segment at some point to complete a sub-haplotype.
							HapSegment.add2hashmap(matching_segments, new HapSegment(the_seg_start, the_seg_end, the_seg_seq));
						}
						the_seg_start=k+1;	// Move onto the next variant to look at the next linkage indicator. 
						the_seg_end=k+1;  // The next sign must not be a '?', which will increase the_seg_end.
						the_seg_seq=""+gc_outcome2[p_index][h].charAt(k*2+2);
					}else if(gc_outcome2[p_index][h].charAt(k*2+1)=='-'){ // There is no gap, so the segment continues to grow. 
						the_seg_end++;
						the_seg_seq=the_seg_seq+gc_outcome2[p_index][h].charAt(k*2+2);
					}else System.out.println("ERROR: either '?' nor '-' in the GC outcome.");
				}// finally the last one goes to the hashset. 
				if(the_seg_start==region_start){
					HapSegment.add2hashmap(segments_buffer1, new HapSegment(the_seg_start, the_seg_end, the_seg_seq));
				}else{
					HapSegment.add2hashmap(matching_segments, new HapSegment(the_seg_start, the_seg_end, the_seg_seq));
				}
			}
			// Step 2) Extend zero-start segments in buffer1 and put them in buffer2. Do this iteratively until nothing in either storage can be extended. 
			int num_partial_hap1=divide_parameters.max_num_haps+1, num_partial_hap2=divide_parameters.max_num_haps+1;
			int round_index=0;
			// stdout.write("The current pool is " + p_index + ".\n");
			// stdout.write("Extend zero-start segments in buffer1 and put in buffer2, and then vice versa.\n\n");
			do {	// While there are iterations available to finish the sub-haplotypes...
				// stdout.write("====Region_"+region_index+"/ Pool_"+p_index+"/ Round_"+round_index + "\n");
				num_partial_hap2=extend_segments(segments_buffer1, segments_buffer2, matching_segments, region_end, null);
				if(num_partial_hap2==0)break;
				num_partial_hap1=extend_segments(segments_buffer2, segments_buffer1, matching_segments, region_end, null);
				if(num_partial_hap1==0)break;
				round_index++;
			}while(round_index<divide_parameters.max_num_rounds_forming_initial_haps);
			// Step 3) Whichever buffer contains the fully extended sub-haplotypes, add them to the permanent storage. 
			if(num_partial_hap2==0) add2global(segments_buffer2, all_full_haps);
			else if (num_partial_hap1==0) add2global(segments_buffer1, all_full_haps);
			else {  // Not successfully finished i.e.; there may be partial-length sub-haplotypes. Take the latest one (num_partial_hap1) and remove the ones that don't fully span the region.
				for(HapSegment hap_seg: segments_buffer1.keySet()) {
					if(hap_seg.end_index!=region_end) {
						segments_buffer1.remove(hap_seg);
					}
				}
				add2global(segments_buffer1, all_full_haps);
			}
		}
		// Step 4) Use the full-length sub-haplotypes to construct the between-pool HapConfig object.
		int num_site_regional=region_end-region_start+1;
		// assign global haps and their frequencies.
		String[][] global_haps_string=new String[all_full_haps.size()][num_site_regional];
		double[] global_haps_freq=new double[all_full_haps.size()];
		String[] hap_IDs=new String[all_full_haps.size()];
		int hap_index=0;
		double support_sum=0;
		// Step 4a) Get variant information for each sub-haplotype at each position, as well as global haplotype frequency information. 
		for(HapSegment full_hap: all_full_haps.keySet()) {	// For each sub-haplotype in permanent storage...
			String tmp = ""; 
			for(int k=0;k<num_site_regional;k++) {	// ...for each position in the range...
				global_haps_string[hap_index][k]=full_hap.sequence.charAt(k)+"";	// ...get the variant at that position. 
				tmp += Character.toString(full_hap.sequence.charAt(k)); 
			}
			hap_IDs[hap_index] = tmp;	// Made alterations to hap_IDs so that the regions can be combined more easily. 
			global_haps_freq[hap_index]=all_full_haps.get(full_hap);	// ...get the raw amount of support for that haplotype. 
			support_sum+=global_haps_freq[hap_index];					// I think the start of the frequency estimate will have a mid-high mean and a very low variance since it's dominated by support from low-frequency segments.
			hap_index++;
		}for(int h=0;h<all_full_haps.size();h++) {
			global_haps_freq[h]=global_haps_freq[h]/support_sum;		// Divide the raw amount of support by the total amount of support. 
		}
		// Step 4b) Assign site and pool annotation information (ex. names). 
		double[][] inpool_site_freqs=new double[num_site_regional][];
		LocusAnnotation[] locusInfo=new LocusAnnotation[num_site_regional];
		for(int k=0;k<num_site_regional;k++) {
			locusInfo[k]=this.in_pool_sites_freq_anno.loci_annotations[k+region_start];
			inpool_site_freqs[k]=this.in_pool_sites_freq_anno.inpool_freqs[k+region_start];
			// System.out.println("DivideConquer:\t" + k + "\t" + locusInfo[k].alleles_coding.size());
		}
		double[][] in_pool_haps_freq= null;
		String[] pool_IDs=null; 
		
		HapConfig regional_hap_config=new HapConfig(global_haps_string, global_haps_freq, in_pool_haps_freq, inpool_site_freqs, 
				locusInfo, this.num_pools, hap_IDs, pool_IDs, this.divide_parameters.est_ind_pool);
		// stdout.write("There are, in total, " + global_haps_freq.length + " regional haplotypes.");
		// stdout.close();
		return regional_hap_config;
	}
	
	/*
	 * Add one HashMap<HapSegment,Integer> (in one pool) to another one (global)
	 * Note that the global one is the 2nd parameter.
	 */
	
	public static void add2global(HashMap<HapSegment,Integer> single_pool_haps, HashMap<HapSegment,Integer> global_haps) {
		for(HapSegment hap: single_pool_haps.keySet()) {
			if(global_haps.containsKey(hap)) {
				int new_support=global_haps.get(hap)+single_pool_haps.get(hap);
				global_haps.put(hap,new_support);
			}else {
				global_haps.put(hap,single_pool_haps.get(hap));
			}
		}
	}
	
	/*
	 * Match all the zero-start segments with all the matching segments.
	 * the returned integer records how many partial haps are still remained.
	 * 
	 * During the growth, to avoid too many segments, the low-support ones will be trimmed using AemParameters.max_num_haps. 
	 *  
	 * Note that extended_segments will be cleared at the beginning. 
	 */
	// num_partial_hap2=extend_segments(segments_buffer1, segments_buffer2, matching_segments, region_end);
	// num_partial_hap1=extend_segments(segments_buffer2, segments_buffer1, matching_segments, region_end);
	public int extend_segments(HashMap<HapSegment, Integer> ini_segments, HashMap<HapSegment, Integer> extended_segments, 
			HashMap<HapSegment, Integer> matching_segments, int region_end, BufferedWriter stdout) throws IOException {
		int num_partial_hap=0;
		extended_segments.clear();	// If this is the ith iteration of zero-start segment extension, this HashMap contains the (i - 1)th zero-start segments i.e.: smaller sub-haplotypes from the previous round.
		// Step 1) Make all sub-haplotypes from the available segments, and make them as long as possible. 
		// Based on all available gap positions, this is an exhaustive search for sub-haplotypes from all segments. 
		for(HapSegment zerostart_seg: ini_segments.keySet()) {	// For each started sub-haplotype i.e.: zero-start segment...
			if(zerostart_seg.end_index==region_end) {	// If the zero-start segment is finished i.e.: ends at the last position in the region, it is a full sub-haplotype.
				extended_segments.put(zerostart_seg, ini_segments.get(zerostart_seg));	// Just add that to the next storage. 
			}else {
				for(HapSegment matching_seg: matching_segments.keySet()) {
					if(zerostart_seg.end_index+1==matching_seg.start_index) {	// If the zero-start segnment ends just before the matching segment starts, they might be in linkage i.e.: is a potential sub-haplotype. 
						int support=Math.min(ini_segments.get(zerostart_seg), matching_segments.get(matching_seg));	// The sub-haplotype is supported by the raw quantity of one of the segments, whichever is less.
						extended_segments.put(HapSegment.combine(zerostart_seg, matching_seg), support);
						if(matching_seg.end_index!=region_end)num_partial_hap++;	// If this extended zero-start segment doesn't end at the last position in the region, it's still a partial sub-haplotype.
					}
				}
			}			
		}
		// Step 2) Trim low-support sub-haplotypes in the zero-start storage if there are too many. 
		int num_tobe_trimmed = 0;
		if(extended_segments.size()>divide_parameters.max_num_haps) {
			// Sort the sub-haplotypes by the amounnt of support (see line 434) they have...
			HashMap<HapSegment, Integer> sorted_extended_segments = extended_segments.entrySet()
					.stream().sorted(comparingByValue())
					.collect(toMap(e -> e.getKey(), e -> e.getValue(), (e1, e2) -> e2, LinkedHashMap::new));	// TOLEARN
			// ...then remove as many sub-haplotypes as needed to hit the maximum number considerable.
			num_tobe_trimmed=sorted_extended_segments.size()-divide_parameters.max_num_haps;
			int num_removed=0;
			for(HapSegment hap_seg: sorted_extended_segments.keySet()) {
				if(hap_seg.end_index!=region_end) num_partial_hap--;
				extended_segments.remove(hap_seg);
				num_removed++;
				if(num_removed==num_tobe_trimmed) break;
			}
			//System.out.println("debug");
		}
		// stdout.write("total_segments/partial: "+extended_segments.size()+"/"+num_partial_hap + "\n");
		// stdout.write("Trimmed "+num_tobe_trimmed+" segments." + "\n\n");
		return num_partial_hap;
	}
		
	/*
	 * Analyze a set of regions based on the dividing plan. 
	 * 
	 * This function can be used for either level I or level II, or a subset of them 
	 */
	public HapConfig[] analyze_regions(int[][] regions, String parameter_file) throws Exception{
		int num_region=regions.length;
		HapConfig[] region_haps=new HapConfig[num_region];
		for(int r=0;r<1;r++){
			HapConfig hap_config = generate_hapconfig_from_gc(regions, r, this.gc_outcome, this.in_pool_sites_freq_anno);	// Will be extended in the future such that a user-specified sub-sequence can be reconstructed instead.
			// hap_config.write_inpool("C:\\gc\\haps_inpool_" + r + ".txt", true);
			// hap_config.write_global_file_code("C:\\gc\\haps_allpool_" + r + ".txt", true);
			PoolSolver2 hap_solver = new PoolSolver2(hap_config.num_loci, this.num_pools, hap_config, parameter_file); 
			region_haps[r] = hap_solver.report();
			region_haps[r].write_inpool("/home/lmak/Documents/gc/region_1_haps.txt_inpool", false); 
			region_haps[r].write_global_file_code("/home/lmak/Documents/gc/region_1_haps_allpool.txt", false); 
		}
		return region_haps; 
	}
	
	/*
	 * Prepare a document to be used for combining the outcome of level_I and level_II outcome from AEM/rjMCMC //TODO
	 * The gaps of level-I are mostly the gaps in GC. The motivation of having level-II is to connect the gaps 
	 * 	using AEM/rjMCMC (based on LD and allele frequency info).
	 * 
	 * @ DivideParameters.level_I_and_II_alignment_cutoff is used here.
	 * 
	 * The process works as follows:
	 * 	Given any pair of adjacent regions at level_I, R1 and R2, and the region that covers their gap, R'
	 *  we seek the haplotypes that perfectly (or almost perfectly) overlap with both R1 and R2. Then 
	 *  the outcome will be written to the out_file for further algorithm that links the long haplotypes.
	 * 
	 * The outcome is in the following format: 
	 *  In the first block, each line stands for the haplotypes in each region in Level_I
	 *	
	 *	#Compound_sites (half-regions)    
	 *	site_ID1:site_start:site_end \t Hap1_ID:Hap1_seq:Hap1_freq \t Hap2_ID:Hap2_seq:Hap2_freq \t ... \t HapN_ID:HapN_seq:HapN_freq\t (N is the number of haps in this half region)
	 * 	site_ID2:site_start:site_end \t Hap1_ID:Hap1_seq:Hap1_freq \t Hap2_ID:Hap2_seq:Hap2_freq \t ... \t HapN_ID:HapN_seq:HapN_freq\t (N is the number of haps in this half region)
	 * 	....
	 * 	site_ID2m:site_start:site_end \t Hap1_ID:Hap1_seq:Hap1_freq \t Hap2_ID:Hap2_seq:Hap2_freq \t ... \t HapN_ID:HapN_seq:HapN_freq\t (N is the number of haps in this half region)
	 * 
	 * 	Then, in the next block, each line stands for a level_II haplotype (actually a link) that glues two haplotypes 
	 * 	in the two adjacent	regions above:
	 *  
	 * 	#Links between compound sites
	 * 	Site_ID1:Hap_ID1;SiteID2:Hap_ID2;freq_of_this_link \t ...  \t Site_ID1:Hap_ID1;SiteID2:Hap_ID2;freq_of_this_link
	 * 	Site_ID2:Hap_ID1;SiteID2:Hap_ID2;freq_of_this_link \t ...  \t Site_ID1:Hap_ID1;SiteID2:Hap_ID2;freq_of_this_link
	 * 	...
	 * 	Site_IDm:Hap_ID1;SiteID2:Hap_ID2;freq_of_this_link \t ...  \t Site_ID1:Hap_ID1;SiteID2:Hap_ID2;freq_of_this_link
	 */
	public void combine_levels_I_and_II(HapConfig[] level_I, HapConfig[] level_II, String out_file){
		if(level_I.length-1!=level_II.length) {
			System.out.println("ERROR: level_I.length-1!=level_II.length in combine_levels_I_and_II()");
			return;
		}if(this.divide_parameters.level_I_and_II_alignment_cutoff!=0) {
			System.out.println("ERROR: the function for divide_parameters.level_I_and_II_alignment_cutoff!=0 hasn't been implemeted.");
			return; //TODO: this has to be implemented for species with large diversity.
		}
		try {
			// write half-haps and their frequencies to the out_file
			BufferedWriter bw=new BufferedWriter(new FileWriter(out_file));
			Date date = Calendar.getInstance().getTime();  
			DateFormat dateFormat = new SimpleDateFormat("yyyy-mm-dd hh:mm:ss");  
			bw.write("# Aligned haplotypes out of regional analyses."+dateFormat.format(date)+"\n");
			bw.write("#Compound_sites (half-regions)\n");
			// COLLECT HALF HAP AND THEIR FREQUENCY 
			int num_half_hap_sites=level_I.length*2;	// If each sub-haplotype spans a full region, we want half-regions i.e.: where the sub-haplotypes will overlap.

			@SuppressWarnings("unchecked")
			HashMap<String, Double>[] hap_freqs = (HashMap<String, Double>[]) new HashMap[num_half_hap_sites];  // Array of (*) for possible half-segment (see line 537)
			@SuppressWarnings("unchecked")
			HashMap<String, String>[] hap_ids =(HashMap<String, String>[]) new HashMap[num_half_hap_sites];	   // hap_sequence to its ID.
			for(int site_index=0;site_index<num_half_hap_sites;site_index++) {	// For each half-region...
				hap_freqs[site_index]=new HashMap<String, Double>();	// (*) Map of possible level 1 OR 2 haplotypes in each half-segment to their frequencies. 
				if(site_index%2==0) { // Sequence that is in the first half of a level 1 region, and the second half of a level 2 region.
					int start_index=level_I[site_index/2].region[0];
					int end_index=-1;// the start of the level_II region minus 1, which is the middle in the level_I region, assigned below
					if(site_index/2<level_II.length) {
						end_index=level_II[site_index/2].region[0]-1; 
					}else {
						end_index=level_II[site_index/2-1].region[1]; 
					}bw.write("S_"+site_index+":"+start_index+":"+end_index);
					// process level I haps first:
					for(int h1=0;h1<level_I[site_index/2].num_global_hap;h1++) {	// For each level 1 sub-haplotype that overlaps half-segment...
						//make the first half hap:
						String the_hap="";
						for(int k=0;k<=end_index-start_index;k++) {
							the_hap=the_hap+level_I[site_index/2].global_haps_string[h1][k]; //TODO: check whether global_haps_string has been assigned in AEM/rjMCMC!!! 
						}
						// put the level_I half hap into the hashmap. 
						double the_hap_freq=level_I[site_index/2].global_haps_freq[h1];
						if(hap_freqs[site_index].containsKey(the_hap)) {
							double the_added_freq=hap_freqs[site_index].get(the_hap)+the_hap_freq;
							hap_freqs[site_index].put(the_hap, the_added_freq);
						}else {
							hap_freqs[site_index].put(the_hap, the_hap_freq);
						}
					}
					// then process level II haps: 
					if(site_index==0)continue; // the beginning, so no level_II at all, go to the next region.
					// or, there is a level_II region, cut its 2nd half
					for(int h2=0;h2<level_II[site_index/2-1].num_global_hap;h2++) {
						//make the second half hap:
						String the_hap="";
						int the_hap_len=level_II[site_index/2-1].num_loci;
						int the_half_len=end_index-start_index+1;
						int the_middle_index=the_hap_len-the_half_len;
						for(int k=the_middle_index;k<the_hap_len;k++) {
							the_hap=the_hap+level_II[site_index/2-1].global_haps_string[h2][k]; //TODO: check whether global_haps_string has been assigned in AEM/rjMCMC!!! 
						}
						// put the level_II half hap into the hashmap. 
						double the_hap_freq=level_II[site_index/2-1].global_haps_freq[h2];
						if(hap_freqs[site_index].containsKey(the_hap)) {
							double the_added_freq=hap_freqs[site_index].get(the_hap)+the_hap_freq;	// QUESTION: If we're summing the frequencies of half-segments from level 1 and 2, won't half-segments coming up in 1 and 2 be double-counted? Frequency is going to be way off.  
							hap_freqs[site_index].put(the_hap, the_added_freq);
						}else {
							hap_freqs[site_index].put(the_hap, the_hap_freq);
						}
					}
				}else { // (site_index%2!=0) a second half in level_I, and first half in level_II
					int end_index=level_I[site_index/2].region[1];
					int start_index=-1;// the start of the level_II region, which is the middle in the level_I region, assigned below
					if(site_index/2<level_II.length) {	// !!!
						start_index=level_II[site_index/2].region[0];
					}else { //site_index/2==level_II.length
						start_index=level_II[site_index/2-1].region[1]+1; 
					}bw.write("S_"+site_index+":"+start_index+":"+end_index);
					// process level I haps first:
					for(int h1=0;h1<level_I[site_index/2].num_global_hap;h1++) {
						//make the first half hap:
						String the_hap="";
						int the_hap_len=level_I[site_index/2].num_loci;
						int the_half_len=end_index-start_index+1;
						int the_middle_index=the_hap_len-the_half_len;
						for(int k=the_middle_index;k<the_hap_len;k++) {
							the_hap=the_hap+level_I[site_index/2].global_haps_string[h1][k]; //TODO: check whether global_haps_string has been assigned in AEM/rjMCMC!!! 
						}
						// put the level_I half hap into the hashmap. 
						double the_hap_freq=level_I[site_index/2].global_haps_freq[h1];
						if(hap_freqs[site_index].containsKey(the_hap)) {
							double the_added_freq=hap_freqs[site_index].get(the_hap)+the_hap_freq;
							hap_freqs[site_index].put(the_hap, the_added_freq);
						}else {
							hap_freqs[site_index].put(the_hap, the_hap_freq);
						}
					}
					// then process level II haps: 
					if(site_index==num_half_hap_sites-1)continue; // the end, so no level_II at all, go to the next region.
					// or, there is a level_II region, cut its first half
					for(int h2=0;h2<level_II[site_index/2].num_global_hap;h2++) {
						//make the second half hap:
						String the_hap="";
						for(int k=0;k<=end_index-start_index;k++) {
							the_hap=the_hap+level_II[site_index/2].global_haps_string[h2][k]; //TODO: check whether global_haps_string has been assigned in AEM/rjMCMC!!! 
						}
						// put the level_II half hap into the hashmap. 
						double the_hap_freq=level_II[site_index/2].global_haps_freq[h2];
						if(hap_freqs[site_index].containsKey(the_hap)) {
							double the_added_freq=hap_freqs[site_index].get(the_hap)+the_hap_freq;
							hap_freqs[site_index].put(the_hap, the_added_freq);
						}else {
							hap_freqs[site_index].put(the_hap, the_hap_freq);
						}
					}					
				}
				int half_hap_index=0;
				hap_ids[site_index]=new HashMap<String, String>();
				for(String half_hap: hap_freqs[site_index].keySet()) {
					String hap_id="half_Hap_"+half_hap_index++;
					hap_ids[site_index].put(half_hap, hap_id);
					bw.write("\t"+hap_id+":"+half_hap+":"+hap_freqs[site_index].get(half_hap)/2.0);
				}bw.write("\n");
			} 
			// THEN CREATE LINKS
			//Site_ID1:Hap_ID1;SiteID2:Hap_ID2;freq_of_this_link \t ...  \t Site_ID1:Hap_ID1;SiteID2:Hap_ID2;freq_of_this_link
			bw.write("#Links between compound sites\n");
			// find and output the links in level_I first
			for(int r1=0;r1<level_I.length;r1++) {   
				int middle_index=-1; // middle_index is the index of the starting site of the 2nd half. 
				if(r1<level_I.length-1) { // not the last one
					middle_index=level_II[r1].region[0];
				}else {  // the last one
					middle_index=level_II[r1-1].region[1]+1;	// QUESTION Isn't level_II.length = level_I.length - 1? In this case, the last index should be level_I.length - 2? 
				}
				int start_2nd_half=middle_index-level_I[r1].region[0];
				int end_1st_half=start_2nd_half-1;	// QUESTION What if this is the 0th level 2 half-segment? Need to build in a null string to in the HashMap[] to account for this.
				for(int h1=0;h1<level_I[r1].num_global_hap;h1++) { // For each sub-haplotype possibility in that region... 
					//TODO find the best matches if (divide_parameters.level_I_and_II_alignment_cutoff!=0). Here I only implement perfect match (Quan Jan 9th, 2019).
					String hap_1st_half="", hap_2nd_half="";
					for(int k=0;k<=end_1st_half;k++) {
						hap_1st_half=hap_1st_half+level_I[r1].global_haps_string[h1][k];
					}
					String hap_id_1st=hap_ids[r1*2].get(hap_1st_half);  // r1*2 is the index for the 1st half_hap site.
					for(int k=start_2nd_half;k<level_I[r1].global_haps_string[h1].length;k++) {
						hap_2nd_half=hap_2nd_half+level_I[r1].global_haps_string[h1][k];
					}
					String hap_id_2nd=hap_ids[r1*2+1].get(hap_2nd_half);  // r1*2+1 is the index for the 2nd half_hap site.
					if(h1!=level_I[r1].num_global_hap-1) {
						bw.write("S_"+(r1*2)+":"+hap_id_1st+";"+"S_"+(r1*2+1)+":"+hap_id_2nd+";"+level_I[r1].global_haps_freq[h1]+"\t");	// QUESTION Why are we outputting the global_haps_freq of h1? Shouldn't we be outputting the frequency of each half-segment? 
					}else {  // the last one, newline. 
						bw.write("S_"+(r1*2)+":"+hap_id_1st+";"+"S_"+(r1*2+1)+":"+hap_id_2nd+";"+level_I[r1].global_haps_freq[h1]+"\n"); 
					}
				}
			}
			// then find and output the links in level_II
			for(int r2=0;r2<level_II.length;r2++) {   
				int middle_index=level_I[r2].region[1]+1; // middle_index is the index of the starting site of the 2nd half. 
				int start_2nd_half=middle_index-level_II[r2].region[0];
				int end_1st_half=start_2nd_half-1;
				for(int h2=0;h2<level_II[r2].num_global_hap;h2++) { 
					//TODO find the best matches if (divide_parameters.level_I_and_II_alignment_cutoff!=0). Here I only implement perfect match (Quan Jan 9th, 2019).
					String hap_1st_half="", hap_2nd_half="";
					for(int k=0;k<=end_1st_half;k++) {
						hap_1st_half=hap_1st_half+level_II[r2].global_haps_string[h2][k];
					}
					String hap_id_1st=hap_ids[r2*2+1].get(hap_1st_half);  // r2*2+1 is the index for the 1st half_hap site.
					for(int k=start_2nd_half;k<level_II[r2].global_haps_string[h2].length;k++) {
						hap_2nd_half=hap_2nd_half+level_II[r2].global_haps_string[h2][k];
					}
					String hap_id_2nd=hap_ids[r2*2+2].get(hap_2nd_half);  // r2*2+2 is the index for the 2nd half_hap site.
					if(h2!=level_II[r2].num_global_hap-1) {
						bw.write("S_"+(r2*2+1)+":"+hap_id_1st+";"+"S_"+(r2*2+2)+":"+hap_id_2nd+";"+level_II[r2].global_haps_freq[h2]+"\t");
					}else {  // the last one, newline. 
						bw.write("S_"+(r2*2+1)+":"+hap_id_1st+";"+"S_"+(r2*2+2)+":"+hap_id_2nd+";"+level_II[r2].global_haps_freq[h2]+"\n"); 
					}
				}
			}
			bw.close();
		}catch(Exception e){e.printStackTrace();}
	}
	
	public int mismatches(String[] hap_seg1, int start1, int end1, 
			String[] hap_seg2, int start2, int end2) {
		if(end1-start1!=end2-start2) {
			System.out.println("ERROR: end1-start1!=end2-start2 in function mismatches()!");
			return (end1-start1+1)+(end2-start2+1);
		}
		int mismatch_count=0;
		for(int k=start1;k<=end1;k++) {
			if(!hap_seg1[k].equals(hap_seg2[k-start1+start2]))
				mismatch_count++;
		}
		return mismatch_count; 
	}

	/*
	 * Output the current dividing plan to a file
	 */
	public void output_current_DC_plan(String dc_plan_outfile){
		try{
			BufferedWriter bw=new BufferedWriter(new FileWriter(dc_plan_outfile));
			// Output Level I:
			bw.write("Level_I\n");
			for(int r=0;r<this.num_regsion_level_I;r++){
				bw.write(this.regions_level_I[r][0]+":"+this.regions_level_I[r][1]+"\t");
			}
			// Output Level II:
			bw.write("\nLevel_II\n");
			for(int r=0;r<this.num_regsion_level_II;r++){
				bw.write(this.regions_level_II[r][0]+":"+this.regions_level_II[r][1]+"\t");
			}bw.write("\n");
			bw.close();
		}catch(Exception e){e.printStackTrace();}
	}
	
	/*
	 * Output the current dividing plan to a the stdout
	 */
	public void output_current_DC_plan(){
		// Output Level I:
		System.out.print("Level_I\n");
		for(int r=0;r<this.num_regsion_level_I;r++){
			System.out.print(this.regions_level_I[r][0]+":"+this.regions_level_I[r][1]+"\t");
		}
		// Output Level II:
		System.out.print("\nLevel_II\n");
		for(int r=0;r<this.num_regsion_level_II;r++){
			System.out.print(this.regions_level_II[r][0]+":"+this.regions_level_II[r][1]+"\t");
		}System.out.print("\n");		
	}	
}
